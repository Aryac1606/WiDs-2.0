import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#importing data
from google.colab import files
uploaded = files.upload()

df = pd.read_csv('tesla.csv')
print(df.columns.tolist())

# Cleaning and processing data
from sklearn.preprocessing import MinMaxScaler

# 1. Use the 'Close' column from your cleaned df
data = df_ffill.filter(['Close'])
dataset = data.values

# 2. Normalize the data (Crucial for LSTM)
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(dataset)

# 3. Create Training and Testing sets (80/20 split)
training_data_len = int(np.ceil(len(dataset) * .8))
train_data = scaled_data[0:int(training_data_len), :]

# Create the training data set
x_train, y_train = [], []

for i in range(60, len(train_data)):
    x_train.append(train_data[i-60:i, 0])
    y_train.append(train_data[i, 0])

# Convert to numpy arrays and reshape for LSTM [samples, time steps, features]
x_train, y_train = np.array(x_train), np.array(y_train)
x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))

#building and training lstm model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout

model_lstm = Sequential([
    LSTM(50, return_sequences=True, input_shape=(x_train.shape[1], 1)),
    Dropout(0.2),
    LSTM(50, return_sequences=False),
    Dropout(0.2),
    Dense(25),
    Dense(1)
])

# Compile the model
model_lstm.compile(optimizer='adam', loss='mean_squared_error')

# Train the model (using 10% of training data for validation)
history = model_lstm.fit(x_train, y_train, batch_size=32, epochs=20, validation_split=0.1, verbose=1)

# Prediction and reversing the scale
# Create the testing data set
test_data = scaled_data[training_data_len - 60: , :]
x_test, y_test = [], dataset[training_data_len:, :]

for i in range(60, len(test_data)):
    x_test.append(test_data[i-60:i, 0])

x_test = np.array(x_test)
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))

# Get predictions
lstm_predictions = model_lstm.predict(x_test)
lstm_predictions = scaler.inverse_transform(lstm_predictions) # Undo scaling

#final comparison and graphical presentation
from sklearn.metrics import mean_absolute_percentage_error

# Calculate LSTM Metrics
lstm_rmse = np.sqrt(mean_squared_error(y_test, lstm_predictions))
lstm_mae = mean_absolute_error(y_test, lstm_predictions)
lstm_mape = mean_absolute_percentage_error(y_test, lstm_predictions) * 100

print(f"LSTM Metrics -> RMSE: {lstm_rmse:.2f}, MAE: {lstm_mae:.2f}, MAPE: {lstm_mape:.2f}%")

# Plot Learning Curves (Loss)
plt.figure(figsize=(8, 4))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('LSTM Learning Curves')
plt.legend()
plt.show()

# MASTER COMPARISON PLOT
plt.figure(figsize=(16, 8))
plt.plot(test.index, y_test, label='Actual Price', color='black', alpha=0.7)
plt.plot(test.index, forecast_mean, label='ARIMA Prediction', color='red', linestyle='--')
plt.plot(test.index, lstm_predictions, label='LSTM Prediction', color='green')
plt.title('Model Comparison: ARIMA vs LSTM')
plt.legend()
plt.show()
